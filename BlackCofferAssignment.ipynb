{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing All the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import hyphenate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create stop words list and positive and negative word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_folder = \"D:\\\\Beep Beep Boop Boop\\\\Python\\\\BlackCoffer Internship Assignment\\\\20211030 Test Assignment\\\\StopWords\"\n",
    "all_stopwords_list = []\n",
    "\n",
    "for file_name in os.listdir(stopwords_folder):\n",
    "    with open(os.path.join(stopwords_folder, file_name), 'r') as file:\n",
    "        all_stopwords_list.extend(file.read().splitlines())\n",
    "# all_stopwords_list=set(all_stopwords_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words_path = \"D:\\\\Beep Beep Boop Boop\\\\Python\\\\BlackCoffer Internship Assignment\\\\20211030 Test Assignment\\\\MasterDictionary\\\\negative-words.txt\"\n",
    "negative_words_path = \"D:\\\\Beep Beep Boop Boop\\\\Python\\\\BlackCoffer Internship Assignment\\\\20211030 Test Assignment\\\\MasterDictionary\\\\positive-words.txt\"\n",
    "\n",
    "with open(positive_words_path, 'r') as positive_file:\n",
    "    positive_words = positive_file.read().splitlines()\n",
    "\n",
    "with open(negative_words_path, 'r') as negative_file:\n",
    "    negative_words = negative_file.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the link which you want to extract the text from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://insights.blackcoffer.com/database-discovery-tool-using-openai/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the article text and the title using beautiful soup html parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# Extract the title of the article from the <h1> tag\n",
    "title_tag = soup.find('h1')\n",
    "title = title_tag.text.strip() if title_tag else \"Untitled\"\n",
    "\n",
    "potential_class_ids = ['td-post-content tagdiv-type', 'tdb-block-inner td-fix-index']\n",
    "\n",
    "for class_id in potential_class_ids:\n",
    "    text_elements = soup.find_all(class_=class_id)\n",
    "    if text_elements:\n",
    "        break\n",
    "\n",
    "# Extract text from each element\n",
    "text = []\n",
    "\n",
    "for element in text_elements:\n",
    "    # Remove '\\n' tags\n",
    "    element_text = element.get_text(separator='\\n', strip=True)\n",
    "\n",
    "    # Remove content within <pre> tag\n",
    "    pre_tag = element.find('pre')\n",
    "    if pre_tag:\n",
    "        element_text = element_text.replace(pre_tag.get_text(strip=True), '')\n",
    "\n",
    "    text.append(element_text)\n",
    "\n",
    "# Join the text from all elements into a single string\n",
    "final_text = '\\n'.join(text)\n",
    "\n",
    "# Include the title along with the content\n",
    "final_content = f\"Title: {title}\\n\\n{final_text}\"\n",
    "final_content = final_content.replace('\\n',' ') #to remove all \\n characters that get parsed with the article paragraphs\n",
    "# print(final_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Database Discovery Tool using OpenAI  Client Background Client: A leading retail firm in the USA Industry Type: Retail Products & Services: Retail Business, e-commerce Organization Size: 100+ Problem Statement: Organizations often face challenges in managing and understanding their vast and complex databases. As data infrastructure evolves, new databases are introduced, and existing ones are modified, leading to a lack of comprehensive visibility into the entire data landscape. This lack of awareness poses several issues, including increased difficulty in ensuring data quality, security vulnerabilities, and inefficiencies in database administration. To address these challenges, there is a need for a Database Discovery Tool using OpenAI, aimed at providing an automated and intelligent solution for discovering, cataloging, and understanding the various databases within an organization’s ecosystem. Key Problems to Solve: Database Proliferation: Challenge: The rapid growth of databases within an organization makes it challenging to keep track of all data storage systems. Impact: Increased difficulty in managing, securing, and optimizing databases. Data Schema Variability: Challenge: Databases often have diverse schemas, making it hard to understand the structure of stored data. Impact: Inefficient data integration and difficulty in ensuring data consistency across the organization. Limited Metadata Documentation: Challenge: Lack of comprehensive metadata documentation for databases, including information about tables, columns, relationships, and data types. Impact: Time-consuming manual efforts for understanding data structures and dependencies. Security and Compliance Risks: Challenge: Inability to identify and monitor sensitive data across databases may lead to security and compliance risks. Impact: Increased likelihood of data breaches and non-compliance with regulatory standards. Operational Inefficiencies: Challenge: Manual efforts required for discovering and documenting databases result in operational inefficiencies. Impact: Increased workload for database administrators, leading to potential errors and delays. Lack of Intelligent Insights: Challenge: Absence of intelligent insights into database usage patterns, performance metrics, and optimization opportunities. Impact: Missed opportunities for improving database performance and resource utilization. Proposed Solution: Develop an OpenAI-powered Database Discovery Tool that leverages natural language processing (NLP) and machine learning capabilities to automatically discover, catalog, and provide insights into the organization’s databases. The tool should be able to: Automatically scan and identify databases across different environments. Extract and catalog metadata, including schema details, relationships, and data types. Provide intelligent insights into database usage patterns and performance metrics. Identify and classify sensitive data for enhanced security and compliance. Enable efficient search and navigation of the entire database landscape. Support ongoing updates and synchronization with changes in the data infrastructure. By addressing these challenges, the Database Discovery Tool using OpenAI aims to empower organizations with a holistic view of their data landscape, facilitating better management, security, and optimization of databases. Solution Architecture Step by Step Execution Step 1 . Database Support In this step we communicate with different types of databases, like SQL and Oracle. This means it can connect and retrieve information from a variety of database systems using Python, providing users with more flexibility and compatibility across various database environments. Step 2 . Data Extraction In this step we are using python for our Extract, Transform, Load (ETL) processes this involves efficiently reading and extracting data from the connected databases. Python handled the data-related tasks, ensuring a robust and effective extraction process and save the result in csv files which in turn are converted to .db files for sqlite. Step 3 . Fine-Tuning In this step fine-tuning mechanisms to optimize the performance and accuracy of data extraction processes. This Ensures the ETL tool finds data accurately and quickly. Step 4 . Integration with OpenAI In this step we have utilized SQL Agent for communication with OpenAI, By communicating with OpenAI, the SQL agent get the ability to understand and respond in a more intelligent and context-aware manner. Step 5 . API Integration In this step we made Django API endpoints for requesting and receiving data. This means that external systems or applications can interact with the SQL Agent through OpenAI by sending requests and receiving responses through these APIs. Step 6 . Streamlit Frontend In this step we made a streamlit frontend to chat with the SQL Agent. The user can ask question about the database and receive responses in form of insights. Video Demo\n"
     ]
    }
   ],
   "source": [
    "print(final_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = word_tokenize(final_content) #includes stopwords and punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Sentiment Analysis is Performed, We need to remove the stopwords using the list of stop words provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(final_content)\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in all_stopwords_list]\n",
    "# Join the tokens back into a single string\n",
    "processed_text = ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    no_punct = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = remove_punctuation(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(clean_text.lower())  # Convert to lowercase for case-insensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447\n"
     ]
    }
   ],
   "source": [
    "total_clean_words = len(tokens)\n",
    "print(total_clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "# Calculate Positive Score\n",
    "positive_score = sum(1 for word in tokens if word in positive_words)\n",
    "\n",
    "# Calculate Negative Score (multiply by -1)\n",
    "negative_score = -sum(-1 for word in tokens if word in negative_words)\n",
    "\n",
    "print(positive_score)\n",
    "print(negative_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.08333333159722227\n"
     ]
    }
   ],
   "source": [
    "#Polarity Score\n",
    "polarity_score = (positive_score - negative_score)/((positive_score + negative_score) + 0.000001)\n",
    "print(polarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjectivity Score:  0.10738255009534105\n"
     ]
    }
   ],
   "source": [
    "subjectivity_score = (positive_score + negative_score)/((total_clean_words)+ 0.000001)\n",
    "print(\"Subjectivity Score: \",subjectivity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = len(word_tokenize(final_content)) - 2 #-2 to remove the count of Title and the ':' and for a better understanding of sentences, We take count of all words including stop words\n",
    "number_of_sentences = len(sent_tokenize(final_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795\n"
     ]
    }
   ],
   "source": [
    "print(number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.928571428571427\n"
     ]
    }
   ],
   "source": [
    "Average_sentence_length = number_of_words/number_of_sentences\n",
    "print(Average_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "  \"\"\"Counts the number of syllables in a word.\"\"\"\n",
    "  return len(hyphenate.hyphenate_word(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23647798742138365\n"
     ]
    }
   ],
   "source": [
    "syllable_threshold = 2\n",
    "number_of_complex_words = sum(1 for word in all_words if count_syllables(word) > syllable_threshold)\n",
    "percentage_of_complex_words = number_of_complex_words/number_of_words\n",
    "print(percentage_of_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fog_index  = 0.4 * (Average_sentence_length + percentage_of_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_number_of_words_per_sentence = number_of_words/number_of_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_word_count = number_of_complex_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For stopword removal using the stop word package of NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_words = remove_stopwords(final_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punctuation_and_stop_words = remove_punctuation(no_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476\n"
     ]
    }
   ],
   "source": [
    "#for word count first tokenize the filtered text\n",
    "tokens = word_tokenize(no_punctuation_and_stop_words)\n",
    "word_count = len(tokens)\n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_syllables_per_word(article_text):\n",
    "    # Tokenize the article text into words\n",
    "    no_pnct = remove_punctuation(article_text)\n",
    "    words = word_tokenize(no_pnct)\n",
    "\n",
    "    # Count syllables for each word\n",
    "    syllable_counts = [count_syllables(word) for word in words]\n",
    "\n",
    "    # Calculate total syllable count and total word count\n",
    "    total_syllables = sum(syllable_counts)\n",
    "    total_words = len(words)\n",
    "\n",
    "    # Calculate average syllable count per word\n",
    "    average_syllables_per_word = total_syllables / total_words\n",
    "\n",
    "    return average_syllables_per_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_syllables_per_word  = calculate_average_syllables_per_word(final_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def count_personal_pronouns(text):\n",
    "    pronounRegex = re.compile(r'\\b(I|we|my|our|you|your|they|them|ours|(?-i:us))\\b',re.I)\n",
    "    countof_pronouns = len(pronounRegex.findall(text))\n",
    "    return countof_pronouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_of_personal_pronouns = count_personal_pronouns(final_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_word_length(text):\n",
    "    words = text.split()\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    total_words = len(words)\n",
    "\n",
    "    if total_words == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "\n",
    "    avg_word_length = total_characters / total_words\n",
    "    return avg_word_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_word_length = calculate_average_word_length(final_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
